{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# 提取 Car Evaluation 數據集\n",
    "car_evaluation = fetch_ucirepo(id=19)\n",
    "\n",
    "# 提取特徵和目標\n",
    "features = car_evaluation.data.features\n",
    "target = car_evaluation.data.targets\n",
    "\n",
    "# 轉換為 pandas DataFrame\n",
    "df = pd.DataFrame(features, columns=car_evaluation.feature_names)\n",
    "df['target'] = target\n",
    "\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# 分割數據集並重構index\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "y_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# 合併\n",
    "train = pd.concat([X_train, y_train], axis=1)\n",
    "test = pd.concat([X_test, y_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "unacc    1210\n",
       "acc       384\n",
       "good       69\n",
       "vgood      65\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "car_evaluation = fetch_ucirepo(id=19)\n",
    "\n",
    "data = car_evaluation.data.targets.value_counts()\n",
    "\n",
    "# 各類別出現的次數\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "\n",
    "# 將特徵轉換為所需格式，排除 'target' 列\n",
    "features_list = []\n",
    "for index, row in X_train.iterrows():\n",
    "    features = []\n",
    "    for col, value in row.items():\n",
    "        features.append(f\"{col}: {value}\")\n",
    "    features_list.append(features)\n",
    "# 使用 TransactionEncoder 進行編碼\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(features_list).transform(features_list)\n",
    "# 將編碼後的數據轉換為 DataFrame\n",
    "te_df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "# 使用 apriori 算法找出頻繁項集\n",
    "frequent_itemsets = apriori(te_df, min_support=0.01, use_colnames=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatFrequentItemsets(frequent_itemsets):\n",
    "    formatted_itemsets = []\n",
    "    for index, row in frequent_itemsets.iterrows():\n",
    "        itemset = []\n",
    "        for i, item in enumerate(row['itemsets']):\n",
    "            key = item.split(': ')[0]\n",
    "            value = item.split(': ')[1]\n",
    "            itemset.append({key: value})\n",
    "        formatted_itemsets.append(itemset)\n",
    "    return formatted_itemsets\n",
    "\n",
    "\n",
    "format_frequent_itemsets = formatFrequentItemsets(frequent_itemsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 特徵的頻繁項集與原始資料集做配對\n",
    "# 2. 將包含該項集的樣本取其類別與項集做配對\n",
    "# 3. 形成特徵->類別的形式的規則 {feature: (itemset), target: (class), support: (support), confidence: (confidence)}\n",
    "\n",
    "\n",
    "# 定義函數來生成規則\n",
    "# target_with_count = df['target'].value_counts()\n",
    "\n",
    "\n",
    "def generate_rule(dataset, frequent_itemset):\n",
    "    targets = dataset['target'].unique()\n",
    "    rule = None\n",
    "    max_harmonic_mean = 0\n",
    "    for target in targets:\n",
    "        candidate_rule = {'features': frequent_itemset, 'target': target}\n",
    "        support, confidence, lift = computeSupportAndConfidenceAndLift(\n",
    "            dataset, candidate_rule)\n",
    "        if (confidence >= 0.5 and lift >= 1):\n",
    "            # harmonic_mean = 2 / (1/confidence + 1/lift)\n",
    "            harmonic_mean = confidence\n",
    "            if harmonic_mean > max_harmonic_mean:\n",
    "                max_harmonic_mean = harmonic_mean\n",
    "                rule = candidate_rule\n",
    "                rule['support'] = support\n",
    "                rule['confidence'] = confidence\n",
    "                rule['lift'] = lift\n",
    "                rule['hm'] = harmonic_mean\n",
    "    return rule\n",
    "\n",
    "\n",
    "def check_itemset_in_instance(itemset, instance):\n",
    "    for item in itemset:\n",
    "        item_keys = item.keys()\n",
    "        for item_key in item_keys:\n",
    "            if item_key not in instance:\n",
    "                return False\n",
    "            if item_key in instance and instance[item_key] != item[item_key]:\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def computeSupportAndConfidenceAndLift(df, rule):\n",
    "    features = rule['features']\n",
    "    target = rule['target']\n",
    "\n",
    "    condition = df['target'] == target\n",
    "    condition_without_target = None\n",
    "    for item in features:\n",
    "        key = list(item.keys())[0]\n",
    "        value = list(item.values())[0]\n",
    "        condition = condition & (df[key] == value)\n",
    "        condition_without_target = condition_without_target & (\n",
    "            df[key] == value) if condition_without_target is not None else (df[key] == value)\n",
    "    if df[condition_without_target].shape[0] == 0:\n",
    "        return 0, 0, 0\n",
    "    support = df[condition].shape[0] / df.shape[0]\n",
    "    confidence = df[condition].shape[0] / df[condition_without_target].shape[0]\n",
    "    lift = confidence / (df[df['target'] == target].shape[0] / df.shape[0])\n",
    "    return support, confidence, lift\n",
    "\n",
    "\n",
    "rules = []\n",
    "\n",
    "for i in range(len(format_frequent_itemsets)):\n",
    "    # print(f\"Processing {i+1}/{len(format_frequent_itemsets)}\")\n",
    "    frequent_itemset = format_frequent_itemsets[i]\n",
    "    new_rule = generate_rule(train, frequent_itemset)\n",
    "    if new_rule:\n",
    "        rules.append(new_rule)\n",
    "\n",
    "rules = sorted(rules, key=lambda x: x['hm'], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# databset cover\n",
    "\n",
    "def deataset_cover(rules, df):\n",
    "    now_df = df\n",
    "    strong_rules = []\n",
    "    weak_rules = []\n",
    "    for rule in rules:\n",
    "        if now_df.shape[0] == 0:\n",
    "            weak_rules.append(rule)\n",
    "            continue\n",
    "        cover_df = cover(rule, now_df)\n",
    "        if cover_df is not None:\n",
    "            now_df.drop(cover_df.index, inplace=True)\n",
    "            strong_rules.append(rule)\n",
    "        else:\n",
    "            weak_rules.append(rule)\n",
    "\n",
    "    default_class = None\n",
    "\n",
    "    if now_df.shape[0] != 0:\n",
    "        default_class = now_df['target'].value_counts().idxmax()\n",
    "    else:\n",
    "        # strong_rules裡面最多的target\n",
    "        if len(strong_rules) != 0:\n",
    "            default_class = max(strong_rules, key=lambda x: x['target'])\n",
    "        else:\n",
    "            default_class = max(weak_rules, key=lambda x: x['target'])\n",
    "\n",
    "    if default_class is None:\n",
    "        default_class = df['target'].value_counts().idxmax()\n",
    "\n",
    "    return strong_rules, weak_rules, default_class\n",
    "\n",
    "\n",
    "def cover(rule, df):\n",
    "    features = rule['features']\n",
    "    target = rule['target']\n",
    "    condition = df['target'] == target\n",
    "    for item in features:\n",
    "        key = list(item.keys())[0]\n",
    "        value = list(item.values())[0]\n",
    "        condition = condition & (df[key] == value)\n",
    "    if df[condition].shape[0] == 0:\n",
    "        return None\n",
    "    return df[condition]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_data.shape[0]: 346\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         acc       0.72      0.77      0.74        83\n",
      "        good       0.00      0.00      0.00        11\n",
      "       unacc       0.92      1.00      0.96       235\n",
      "       vgood       1.00      0.12      0.21        17\n",
      "\n",
      "    accuracy                           0.87       346\n",
      "   macro avg       0.66      0.47      0.48       346\n",
      "weighted avg       0.85      0.87      0.84       346\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wei/cba-wl/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/wei/cba-wl/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/wei/cba-wl/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "train_ = train.copy()\n",
    "strong_rules, weak_rules, default_class = deataset_cover(rules, train_)\n",
    "\n",
    "# print(\"strong_rules:\")\n",
    "# for rule in strong_rules:\n",
    "#     print(rule)\n",
    "\n",
    "# print(\"weak_rules:\")\n",
    "# for rule in weak_rules:\n",
    "#     print(rule)\n",
    "\n",
    "# print(\"default_class:\", default_class)\n",
    "\n",
    "\n",
    "def verify(test_data, strong_rules, weak_rules, default_class):\n",
    "    print(\"test_data.shape[0]:\", test_data.shape[0])\n",
    "    results = []\n",
    "    for i in range(test_data.shape[0]):\n",
    "        check = False\n",
    "        instance = test_data.loc[i, :]\n",
    "        for rule in strong_rules:\n",
    "            if check_itemset_in_instance(rule['features'], instance):\n",
    "                results.append(rule['target'])\n",
    "                check = True\n",
    "                break\n",
    "\n",
    "        if not check:\n",
    "            for rule in weak_rules:\n",
    "                if check_itemset_in_instance(rule['features'], instance):\n",
    "                    results.append(rule['target'])\n",
    "                    check = True\n",
    "                    break\n",
    "\n",
    "        if not check:\n",
    "            results.append(default_class)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "results = verify(test, strong_rules, weak_rules, default_class)\n",
    "\n",
    "\n",
    "classification_report_ = classification_report(y_test, results)\n",
    "\n",
    "print(classification_report_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7000000000000001\n"
     ]
    }
   ],
   "source": [
    "print((1+0.78+0.32)/3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
